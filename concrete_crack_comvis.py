# -*- coding: utf-8 -*-
"""concrete_crack_ComVis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lnzB1je8ThDPKTo5QRI4-m1iscUnWqPy

<h1 align=center><font size = 6>Concrete Crack Classification Using VGG-16 and ResNet50: A Comparative Analysis</font></h1>
<br>
<img src="https://raw.githubusercontent.com/doguilmak/Concrete-Classification-Computer-Vision-CoE/main/assets/concrete_crack_wp_meu_metu.png" height=520 width=1000 alt="GitHub">
<small>Picture Source: <a href="https://github.com/doguilmak">Doğu İlmak GitHub</a>

<br>

<h2>Introduction</h2>

<p>Concrete is widely used in construction due to its strength and durability. However, cracks can develop over time, compromising the structural integrity and aesthetics of concrete structures. In recent years, deep learning models have shown promising results in automating crack detection and classification. This scientific presentation focuses on the implementation and comparison of two popular convolutional neural networks (CNNs), VGG-16 and ResNet50, for concrete crack classification. The dataset from <a href="https://data.mendeley.com/datasets/5y9wdsg2zt/2">mendeley</a> is used to train and evaluate the models, with the aim of providing insights into the performance and suitability of these models for crack classification tasks.</p>

<br>

<h2>Data Set</h2>
<p>The dataset contains concrete images having cracks. The data is collected from various <b>METU Campus Buildings</b>. The dataset is divided into two as negative and positive crack images for image classification. Each class has 20000 images with a total of 40000 images with <i>227 x 227</i> pixels with RGB channels. The dataset is generated from 458 high-resolution images (4032x3024 pixel) with the method proposed by Zhang et al (2016). 
<b>High-resolution images have variance in terms of surface finish and illumination conditions. No data augmentation in terms of random rotation or flipping is applied.</b></p>

<br>

<h2>Methodology</h2>

<p>The methodology employed in this study involves the following steps: data collection, preprocessing, model architecture selection, training, and evaluation. The dataset used consists of images of cracked and non-cracked concrete surfaces. Initially, the data is preprocessed by resizing the images  to improve model generalization. VGG-16 and ResNet50, both pre-trained on the ImageNet dataset, are chosen as the base architectures for transfer learning.</p>

<p>Transfer learning is employed to fine-tune the networks on the concrete crack dataset. The last few layers of each model are replaced with fully connected layers, and the entire network is retrained using the dataset. During training, a suitable optimization algorithm and learning rate schedule are selected to ensure convergence and prevent overfitting. The models are then evaluated using various performance metrics, such as accuracy, precision, recall, and F1 score.</p>

<br>

<h2>Acknowledgements</h2>
<p>This dataset has been referred from <a href="https://data.mendeley.com/datasets/5y9wdsg2zt/2">data.mendeley.com</a>. 

<i>If you use this dataset please cite: Özgenel, Çağlar Fırat (2019), “Concrete Crack Images for Classification”, Mendeley Data, V2, doi: 10.17632/5y9wdsg2zt.2</i></p>

<br>

<h2>License</h2>
<p>CC BY 4.0
The files associated with this dataset are licensed under a Creative Commons Attribution 4.0 International license.
What does this mean?

You can share, copy and modify this dataset so long as you give appropriate credit, provide a link to the CC BY license, and indicate if changes were made, but you may not do so in a way that suggests the rights holder has endorsed you or your use of the dataset. Note that further permission may be required for any content within the dataset that is identified as belonging to a third party.</p>

<br>

<h2>Objective:</h2>
<ul>
  <li>Understand the dataset.</li>
  <li>Build classification models to predict the concrete class.</li>
  <li>Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms.</li>
  <li>Evaluate the models.</li>
</ul>

<br>
<h2>Keywords</h2>
<ul>
  <li>Computer Science</li>
  <li>Classification</li>
  <li>Structure</li>
  <li>Neural Networks</li>
  <li>Concrete (Composite Building Material)</li>
  <li>Concrete Cracking</li>
</ul>
<br>

<h2>Content</h2>

<div class="alert alert-block alert-info" style="margin-top: 20px">

<li><a href="https://#">Initial Steps</a></li>
<li><a href="https://#">Dataset Preprocessing</a></li>
<li><a href="https://#">Train Models</a></li>
<li><a href="https://#">Evaluate and Compare Models</a></li>
<li><a href="https://#">Upload and Predict Your Picture!</a></li>
<li><a href="https://#">Results and Discussion</a></li>
<li><a href="https://#">Conclusion</a></li>
<br>

<p></p>
Estimated Time Needed: <strong>30 min</strong>
</div>

<a name="0"></a>
## 0. Initial Steps

<a name="0-1"></a>
### 0.1 Choose the GPU Runtime

- Make sure your runtime is **GPU** (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`.

<a name="0-2"></a>
### 0.2 Downloading Data Set
"""

!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip

!unzip -q concrete_data_week4.zip

"""<a name="0-3"></a>
### 0.3 Importing Libraries
"""

#from tensorflow.keras.applications.resnet50 import preprocess_input
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.callbacks import TensorBoard

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential, Model, load_model
from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, GlobalAveragePooling2D
from keras.applications.vgg16 import preprocess_input

from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import os

import datetime

"""<a name="1"></a>
## 1. Data Preprocessing

<a name="1-1"></a>
### 1.1 Define hyperparameters
"""

#@markdown ---
#@markdown ### Enter number of classes:
num_classes = 1 #@param {type:"integer"}

#@markdown 

#@markdown ### Enter image size:
image_resize = 224 #@param {type:"integer"}

#@markdown

#@markdown ### Enter number of epochs:
num_epochs = 2 #@param {type:"integer"}

#@markdown

#@markdown ### Enter learning rate:
LR = 0.001 #@param {type:"number"}

#@markdown

#@markdown ### Enter batch sizes:
batch_size_training = 128 #@param {type:"integer"}
batch_size_validation = 64 #@param {type:"integer"}
batch_size_test = 32 #@param {type:"integer"}

#@markdown 

#@markdown ### Enter director paths:
train_path = '/content/train' #@param {type:"string"}
valid_path = '/content/val' #@param {type:"string"}
test_path = '/content/test' #@param {type:"string"}
#@markdown ---

"""<p>The generator will run through your image data and apply random transformations to each individual image as it is passed to the model so that it <b>never sees the exact same image twice</b> during training.</p>"""

data_generator = ImageDataGenerator(preprocessing_function=preprocess_input, rescale=1./255)

"""<a name="1-2"></a>
### 1.2 Build train data
"""

train_generator = data_generator.flow_from_directory(
  "concrete_data_week4/train",
  target_size=(image_resize, image_resize),
  batch_size=batch_size_training,
  class_mode='binary')

print('Class indices:', train_generator.class_indices)
print('Number of train samples:', train_generator.samples)
print('First and last 5 filenames:', train_generator.filenames[0:5], train_generator.filenames[-5:])

labels = train_generator.classes
unique_labels, label_counts = np.unique(labels, return_counts=True)

print("Number of unique labels:", len(unique_labels))
for label, count in zip(unique_labels, label_counts):
    print("Label:", label, "- Count:", count)

first_batch = train_generator.next()
first_batch

fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(20, 10))

ind = 0
for ax1 in axs: 
    image_data = first_batch[0][ind]
    ax1.imshow(image_data)
    ind += 1

fig.suptitle('First 4 Images of Train Batch') 
plt.show()

"""<a name="1-3"></a>
### 1.3 Build validation data
"""

validation_generator = data_generator.flow_from_directory(
  "concrete_data_week4/valid",
  target_size=(image_resize, image_resize),
  batch_size=batch_size_validation,
  class_mode='binary')

print('Class indices:', validation_generator.class_indices)
print('Number of validation samples:', validation_generator.samples)
print('First and last 5 filenames:', validation_generator.filenames[0:5], validation_generator.filenames[-5:])

labels = validation_generator.classes
unique_labels, label_counts = np.unique(labels, return_counts=True)

print("Number of unique labels:", len(unique_labels))
for label, count in zip(unique_labels, label_counts):
    print("Label:", label, "- Count:", count)

first_batch = validation_generator.next()
first_batch

fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(20, 10))

ind = 0
for ax1 in axs: 
    image_data = first_batch[0][ind]
    ax1.imshow(image_data)
    ind += 1

fig.suptitle('First 4 Images of Validation Batch') 
plt.show()

"""<a name="2"></a>
## 2. Train Models

<p>In the context of training a machine learning model using batches of data, these lines of code calculate the number of steps or iterations needed to complete an epoch (a full pass through the training or validation dataset) during model training. In the code snippet, it is assumed that you are using data generators (<code>train_generator</code> and <code>validation_generator</code>) to feed the training and validation data to your model in batches. Data generators are commonly used in deep learning tasks when working with large datasets that may not fit entirely into memory.</p>

<p><code>len(train_generator)</code> returns the number of batches or steps required to cover the entire training dataset. Each step involves passing one batch of training data through the model during training. Similarly, <code>len(validation_generator)</code> returns the number of steps required to cover the entire validation dataset during model evaluation.</p>

<p>By assigning these values to <code>steps_per_epoch_training</code> and <code>steps_per_epoch_validation</code>, respectively, you can use these variables to control the number of iterations within an epoch during model training and evaluation. For example, when training the model, you might use a loop that runs for <code>steps_per_epoch_training</code> iterations, with each iteration processing one batch of training data. This ensures that the model trains on the complete training dataset once per epoch. Similarly, during validation or evaluation, you can use <code>steps_per_epoch_validation</code> to specify the number of iterations to cover the entire validation dataset.</p>

<p><b>These steps or iterations are crucial for managing the training and evaluation processes, allowing you to leverage the benefits of batching and efficiently process large datasets while training your model.</b></p>
"""

steps_per_epoch_training = len(train_generator)
steps_per_epoch_validation = len(validation_generator)

"""<a name="2.1"></a>
### 2.1 Train with ResNet-50 architecture

<p>ResNet-50 is a deep convolutional neural network architecture that addresses the problem of vanishing gradients in very deep networks. Developed by <b>Microsoft Research</b>, ResNet-50 introduces the concept of residual connections, allowing for the training of significantly deeper networks. It consists of 50 layers, including residual blocks with shortcut connections that skip over some layers. These shortcuts enable the direct flow of gradients during training, facilitating the learning process for deeper layers. ResNet-50 achieves state-of-the-art performance in image classification tasks and has become a widely adopted architecture in computer vision. Its design principles have also influenced the development of subsequent deeper ResNet variants.</p>
"""

from keras.applications import ResNet50

# load_model('/content/classifier_resnet_model.h5')

"""
<a name="2.1.1"></a>
#### 2.1.1 Build the model"""

resnet_50_model = Sequential()

resnet_50_model.add(ResNet50(
  include_top=False,
  pooling='avg',
  weights='imagenet',
))

resnet_50_model.add(Dense(num_classes, activation='sigmoid'))
resnet_50_model.layers

resnet_50_model.layers[0].layers

"""<br>

<p>In deep learning models, such as ResNet-50, the model is typically composed of multiple layers stacked on top of each other. Each layer performs specific operations on the input data and passes it to the next layer. During training, the model learns to adjust the weights and biases of each layer based on the provided training data, aiming to minimize the loss function.</p>

<p>By setting a layer as non-trainable, we essentially prevent it from updating its weights during the training process. In the case of <code>resnet_50_model.layers[0]</code>, we are referring to the first layer of the ResNet-50 model, which is typically the input layer responsible for accepting the input data. Freezing the input layer can be useful in transfer learning scenarios. Transfer learning involves using a pre-trained model, like ResNet-50, that has been trained on a large dataset (e.g., ImageNet). By freezing the input layer, we ensure that the pre-trained weights and learned features in the earlier layers are retained and not modified during our specific training process.</p>
"""

resnet_50_model.layers[0].trainable = False

resnet_50_model.summary()

"""<a name="2.1.2"></a>
#### 2.1.2 Compile the model

<p>In binary classification, where the number of classes $M$ equals 2, Binary Cross-Entropy(BCE) can be calculated as:</p>

$$-{(y\log(p) + (1 - y)\log(1 - p))}$$

<br>

<p>If $M > 2$ (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result. Categorical Cross-Entropy (CCE) calculated as:</p>

$$-\sum_{c=1}^My_{o,c}\log(p_{o,c})$$



*   $M$ - number of classes
*   $log$ - the natural log
*   $y$ - binary indicator (0 or 1) if class label c is the correct classification for observation o
*   $p$ - predicted probability observation o is of class c
"""

resnet_50_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=LR), loss='binary_crossentropy', metrics=['accuracy', 
                                                                                                                tf.keras.metrics.FalseNegatives(),
                                                                                                                tf.keras.metrics.FalsePositives(),
                                                                                                                tf.keras.metrics.TrueNegatives(),
                                                                                                                tf.keras.metrics.TruePositives(),
                                                                                                                tf.keras.metrics.Precision(),
                                                                                                                tf.keras.metrics.Recall()])

get_ipython().system('rm -rf logs')
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = TensorBoard(logdir)

callbacks_list = [tensorboard_callback]

"""
<a name="2.1.3"></a>
#### 2.1.3 Train the model"""

fit_history = resnet_50_model.fit_generator(
  train_generator,
  steps_per_epoch=steps_per_epoch_training,
  epochs=num_epochs,
  validation_data=validation_generator,
  validation_steps=steps_per_epoch_validation,
  callbacks=[callbacks_list],
  verbose=1,
)

loss, accuracy, false_negatives, false_positives, true_negatives, true_positives, precision, recall = resnet_50_model.evaluate(validation_generator, batch_size=batch_size_validation)
print(f"loss: {loss}\n accuracy: {accuracy}\n false_negatives: {false_negatives}\n false_positives: {false_positives}\n true_negatives: {true_negatives}\n true_positives: {true_positives}\n precision: {precision}\n recall: {recall}")

resnet_50_model.save('classifier_resnet_model.h5')

"""
<a name="2.1.4"></a>
#### 2.1.4 Import TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

get_ipython().run_line_magic('tensorboard', '--logdir logs')

"""<a name="2.2"></a>
### 2.2 Train with VGG-16 architecture

<p>VGG-16 is a deep convolutional neural network architecture that gained popularity for its simplicity and effectiveness in image classification tasks. It was developed by the Visual Geometry Group (VGG) at the <b>University of Oxford</b>. VGG-16 consists of 16 layers, including 13 convolutional layers and 3 fully connected layers. It uses small 3x3 filters throughout the convolutional layers, followed by max-pooling layers for downsampling. The architecture's key characteristic is its depth, allowing it to learn complex features and capture fine-grained details in images. VGG-16 has been widely used as a benchmark and a basis for transfer learning in various computer vision applications.</p>
"""

from keras.applications import VGG16

#load_model('/content/classifier_vgg_model.h5')

"""
<a name="2.2.1"></a>
#### 2.2.1 Build the model"""

vgg_16_model = Sequential()

vgg_16_model.add(VGG16(
  include_top=False,
  pooling='avg',
  weights='imagenet',
))

vgg_16_model.add(Dense(num_classes, activation='sigmoid'))
vgg_16_model.layers

vgg_16_model.layers[0].layers

vgg_16_model.layers[0].trainable = False

vgg_16_model.summary()

"""
<a name="2.2.2"></a>
#### 2.2.2 Compile the model"""

vgg_16_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=LR), loss='binary_crossentropy', metrics=['accuracy', 
                                                                                                                tf.keras.metrics.FalseNegatives(),
                                                                                                                tf.keras.metrics.FalsePositives(),
                                                                                                                tf.keras.metrics.TrueNegatives(),
                                                                                                                tf.keras.metrics.TruePositives(),
                                                                                                                tf.keras.metrics.Precision(),
                                                                                                                tf.keras.metrics.Recall()])

get_ipython().system('rm -rf logs')
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = TensorBoard(logdir)

callbacks_list = [tensorboard_callback]

"""
<a name="2.2.3"></a>
#### 2.2.3 Train the model"""

fit_history = vgg_16_model.fit_generator(
  train_generator,
  steps_per_epoch=steps_per_epoch_training,
  epochs=num_epochs,
  validation_data=validation_generator,
  validation_steps=steps_per_epoch_validation,
  callbacks=[callbacks_list],
  verbose=1,
)

loss, accuracy, false_negatives, false_positives, true_negatives, true_positives, precision, recall = vgg_16_model.evaluate(validation_generator, batch_size=batch_size_validation)
print(f"loss: {loss}\n accuracy: {accuracy}\n false_negatives: {false_negatives}\n false_positives: {false_positives}\n true_negatives: {true_negatives}\n true_positives: {true_positives}\n precision: {precision}\n recall: {recall}")

vgg_16_model.save('classifier_vgg_model.h5')

"""
<a name="2.2.4"></a>
#### 2.2.4 Import TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

get_ipython().run_line_magic('tensorboard', '--logdir logs')

"""<a name="3"></a>
## 3. Evaluate and Compare Models

<p>Evaluating a CNN model is a crucial step in the development of a deep learning project. It helps in understanding how well the model performs on unseen data and provides insights into whether the model is overfitting or underfitting. Evaluation of the CNN model is important for the following reasons:</p>

<br>

<ol>
  <li><b>Performance measurement:</b> Evaluating a model helps in measuring its performance on the test data. This gives an idea of how well the model has learned to generalize to new data.</li>

  <li><b>Model selection:</b> Evaluation of different models can help in selecting the best model among them. It is essential to choose the model with the best performance metrics to deploy it in a production environment.</li>

  <li><b>Hyperparameter tuning:</b> Evaluating a model can also help in fine-tuning the hyperparameters of the model. By comparing the performance of models with different hyperparameters, we can identify the optimal hyperparameters that produce the best performance.</li>

  <li><b>Debugging:</b> Evaluation helps in identifying and diagnosing issues in the model. It can help in determining whether the model is overfitting or underfitting, which can be addressed by adjusting the architecture or hyperparameters.</li>
</ol>
"""

test_generator = data_generator.flow_from_directory(
  '/content/concrete_data_week4/test',
  target_size=(image_resize, image_resize),
  shuffle=False,
  class_mode='binary')

"""<a name="3.1"></a>
### 3.1 Predict test and make dataframe
"""

filenames=test_generator.filenames

pred_resnet_50=resnet_50_model.predict_generator(test_generator, steps=len(test_generator), verbose=1).round(3)
pred_vgg_16=vgg_16_model.predict_generator(test_generator, steps=len(test_generator), verbose=1).round(3)

test_generator.class_indices

filenames_df = pd.DataFrame(filenames, columns=['File Path'])
pred_resnet_df = pd.DataFrame(np.round(pred_resnet_50), columns=['ResNet-50 Predicted Indice'])
pred_vgg_df = pd.DataFrame(np.round(pred_vgg_16), columns=['VGG-16 Predicted Indice'])
model_predictions = pd.concat([filenames_df, pred_resnet_df, pred_vgg_df], axis=1)
model_predictions

file_name='model_predictions.csv'
model_predictions.to_csv(file_name, sep=',', encoding='utf-8')

"""<a name="3.2"></a>
### 3.2 Confusion matrix

<p>Confusion Matrix is a performance measurement for machine learning classification.</p>

<img src="https://miro.medium.com/v2/resize:fit:1400/1*fxiTNIgOyvAombPJx5KGeA.png" height=450 width=500>
"""

y_true = test_generator.classes

"""<a name="3.2.1"></a>
#### 3.2.1 Confusion matrix of ResNet-50 model
"""

y_pred_resnet = resnet_50_model.predict(test_generator)

y_pred_resnet = np.round(y_pred_resnet)

cm = confusion_matrix(y_true, y_pred_resnet)

print("ResNet-50 Confusion Matrix:")
print(cm)

"""<a name="3.2.2"></a>
#### 3.2.2 Confusion matrix of VGG-16 model
"""

y_pred_vgg = vgg_16_model.predict(test_generator)

y_pred_vgg = np.round(y_pred_vgg)

cm = confusion_matrix(y_true, y_pred_vgg)

print("VGG-16 Confusion Matrix:")
print(cm)

"""<a name="3.3"></a>
### 3.3 Classification Report

<p>A classification report is a performance evaluation metric in machine learning. It is used to show the precision, recall, F1 Score, and support of your trained classification model.</p>

<br>

*  **Precision**: Precision indicates how well the model correctly identifies the negative class. It tells you the proportion of samples that are predicted as negative and actually negative. High precision means that when the model predicts a sample negatively, it will likely be correct.

<br>

$$Precision = \frac{TP}{(TP + FP)}$$

<br>

*  **Recall**: The recall shows how well the model caught all negative samples in the data. It tells you the proportion of true negative samples that the model predicts as negative. A high recall means the pattern is good at finding positive examples and less likely to miss them.

<br>

$$Recall = \frac{TP}{(TP + FN)}$$

<br>

* **F1**: The F1 score is a measure of a model's accuracy that combines both precision and recall into a single metric. The F1 score is calculated as the harmonic mean of precision and recall. The formula for calculating the F1 score is:

$$F1\space score = 2 * \frac{(precision * recall)}{(precision + recall)}$$

<br>

* **Support**: In a classification report, support is typically displayed alongside other metrics such as precision, recall, and F1 score for each class. It provides additional context by showing how many instances were present for each class during the evaluation of the model.

<img src="https://miro.medium.com/v2/resize:fit:878/1*Ub0nZTXYT8MxLzrz0P7jPA.png" height=450 width=800>

<a name="3.3.1"></a>
#### 3.3.1 Classification report of ResNet-50 model
"""

cr = classification_report(y_true, y_pred_resnet, target_names=test_generator.class_indices.keys())

print("Classification Report:")
print(cr)

"""<a name="3.3.2"></a>
#### 3.3.2 Classification report of VGG-16 model
"""

cr = classification_report(y_true, y_pred_vgg, target_names=test_generator.class_indices.keys())

print("Classification Report:")
print(cr)

"""<a name="3.4"></a>
### 3.4 Receiver Operating Characteristic (ROC) and the Area Under the Curve (AUC)

**The Receiver Operating Characteristic (ROC)** curve is a graphical representation of the performance of a binary classification model at various classification thresholds. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) as the classification threshold is varied. The True Positive Rate (TPR), also known as Sensitivity or Recall, represents the proportion of actual positive instances correctly classified as positive by the model. It is calculated as:

$$TPR = \frac{TP}{(TP + FN)}$$

The False Positive Rate (FPR) represents the proportion of actual negative instances incorrectly classified as positive by the model. It is calculated as:

$$FPR = \frac{FP}{(FP + TN)}$$

The ROC curve provides a visual way to assess the trade-off between the true positive rate and the false positive rate for different classification thresholds. It helps in understanding how well the model is able to discriminate between the positive and negative classes.

<br>
<br>

**The Area Under the Curve (AUC)** is a single numerical metric that quantifies the overall performance of the ROC curve. It represents the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance according to the model's classification. The AUC ranges from 0 to 1, where a higher value indicates better performance.

Interpreting the AUC:

*  **AUC = 1**: The model has perfect discrimination, meaning it can perfectly distinguish between positive and negative instances.
*  **AUC = 0.5**: The model performs no better than random guessing.
*  **AUC < 0.5**: The model's predictions are worse than random guessing. It might be misclassifying positive and negative instances.

In summary, the ROC curve and AUC provide a comprehensive evaluation of the classification model's performance across different classification thresholds. They are useful for comparing and selecting models, as well as understanding the trade-off between true positive rate and false positive rate. A higher AUC indicates better overall model performance.

<a name="3.4.1"></a>
#### 3.4.1 ROC and AUC score of ResNet-50 model
"""

fpr, tpr, _ = roc_curve(y_true, y_pred_resnet)

roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ResNet-50 Model Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""<a name="3.4.2"></a>
#### 3.4.2 ROC and AUC score of VGG-16 model
"""

fpr, tpr, _ = roc_curve(y_true, y_pred_vgg)

roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('VGG-16 Model Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

from google.colab import files
files.download('/content/classifier_resnet_model.h5') 
files.download('/content/classifier_vgg_model.h5')
files.download('/content/model_predictions.csv')

"""<a name="4"></a>
## 4. Upload and Predict Your Picture

<p>You can upload any image and have the model predict whether there is a concrete crack or not.</p>
<ul>
  <li>Find an image of a concrete or not.</li>
  <li>Run the following code cell.  It will ask you to upload an image.</li>
  <li>The model will print "image has crack" or "image hasn't got any crack" depending on the model's prediction.</li>
<ul>
"""

from google.colab import files
from keras.utils import load_img, img_to_array

#@markdown ---

#@markdown

#@markdown ### Choose model for prediction:

model_selection = 'VGG-16' #@param ["ResNet-50", "VGG-16"]
if model_selection == 'ResNet-50':
  #model = load_model('/content/classifier_resnet_model.h5')
  model = resnet_50_model
elif model_selection == 'VGG-16':
  # model = load_model('/content/classifier_vgg_model.h5')
  model = vgg_16_model

#@markdown

#@markdown ---
from google.colab import files
from keras.utils import load_img, img_to_array
from PIL import Image

# ...

uploaded = files.upload()

for fn in uploaded.keys():
  path = '/content/' + fn
  img = load_img(path, target_size=(image_resize, image_resize))
  
  # Convert the image to RGB format
  img = img.convert('RGB')
  
  x = img_to_array(img)
  x = np.expand_dims(x, axis=0)

  image_tensor = np.vstack([x])
  classes = model.predict(image_tensor)
  classes = np.round(classes)

  print(classes)
  print(classes[0])
  if classes[0] == 1:
    print(fn + " image has a crack.")
  else:
    print(fn + " image doesn't have any crack.")

"""## Results and Discussion

<p>The trained VGG-16 and ResNet50 models are evaluated on a separate test set to measure their performance in concrete crack classification. The results show that both models achieve high accuracy, with VGG-16 obtaining an accuracy of 98% and ResNet50 achieving an accuracy of 94%.</p>

<p>Further analysis reveals that VGG-16 outperforms ResNet-50 in terms of precision, recall, and F1 score, with values of <b>0.98</b>, <b>0.97</b>, and <b>0.98</b>, respectively. ResNet-50 exhibits slightly lower precision, recall, and F1 score values of <b>0.97</b>, <b>0.89</b>, and <b>0.94</b>, respectively. These findings suggest that VGG-16 has a higher ability to accurately classify cracks in concrete images.</p>

<p>The superior performance of VGG-16 can be attributed to its deeper architecture and skip connections, which allow for better feature extraction and information flow through the network. On the other hand, residual connections in ResNet50 help mitigate the vanishing gradient problem, enabling more effective training.</p>

<br>

## Conclusion

<p>In conclusion, this study demonstrates the successful implementation of VGG-16 and ResNet50 for concrete crack classification. Both models achieve high accuracy, but <b>VGG-16</b> exhibits superior precision, recall, and F1 score. These findings highlight the importance of selecting an appropriate CNN architecture for accurate and reliable crack classification in concrete structures.<p>

<h1 align=center><font size = 6>Winner is VGG-16!</font></h1>

<img src="https://media.tenor.com/cyZTfl14frYAAAAC/win-obama.gif" height=400 width=1000 alt="media.tenor.com">

<br>
<br>

<img src="https://i.giphy.com/media/ozQw5uEPDOxrXjLoST/giphy.webp" height=400 width=1000 alt="media.tenor.com">

<h1>Contact Me</h1>
<p>If you have something to say to me please contact me:</p>

<ul>
  <li>Twitter: <a href="https://twitter.com/Doguilmak">Doguilmak</a></li>
  <li>Mail address: doguilmak@gmail.com</li>
</ul>
"""

from datetime import datetime
print(f"Changes have been made to the project on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")